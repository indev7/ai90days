# Refactor Aime Assistant to Scalable Tool-Driven Architecture

## Goal
Refactor the AI assistant into a scalable architecture. Replace the single “GOD prompt” approach with:

- A **base system prompt** (always included)
- An **intent-driven information gathering tool**: `req_more_info`  
  The model uses this tool to request only the minimum extra context it needs (to save tokens).

You will modify these files together:
- `pages/aime2` (frontend UI + request orchestration)
- `api/aime/route.js` (backend orchestration)
- `anthropicHelper.js` (tool wiring + message handling)

> Note: We will implement `openAIHelper` and `ollamaHelper` later. Keep the design extensible so additional providers can plug in with the same tool + context flow.

---

## Step 1 — Define and register the `req_more_info` tool (backend)

### Tool purpose
`req_more_info` is not a “real” external API call. It is a **structured signal** from the LLM telling the app what extra context to attach before re-asking the same user question.

### Requirement
The tool arguments must support the following top-level sections (at least one must be present):
1. `data`
2. `domainKnowledge`
3. `tools`

Attach this tool to the Anthropic tools list in `api/aime/route.js` and ensure the backend can detect and react to it.

---

## Tool schema (must be explicit)

### 1) `data` section
The LLM can request specific parts of `mainTree` to be included for answering the user query.

**Key requirements**
- The LLM must request **only the fields it needs** (token efficient), not the whole tree.
- The tool must support requesting **multiple sections** from `mainTree` in one call (e.g., `MyOKRs` + `SharedOKRs`).
- The LLM can specify field selection paths per section.

**Suggested structure**
- `data.sections[]` where each item includes:
  - `sectionId` (examples: `"MY_OKRS"`, `"SHARED_OKRS"`)
  - `paths[]` (dot-paths or selectors supported by `lib/ai/contextSelector.js`)
  - optional `reason`

#### Frontend behavior (`pages/aime2`)
When the assistant returns a `req_more_info` tool call:

1. Parse the tool args.
2. If `data` is requested:
   - Use `lib/ai/contextSelector.js` to extract only the requested fields from `mainTree`.
   - Attach the extracted data to the next request (preferably into the **system prompt** or a dedicated “context” block).
3. Before attaching:
   - Detect if the same data block is already attached.
   - If attached, **remove and reattach** (ensures freshness and allows newly requested fields to be included).
4. Resend the **same user query** with the augmented context.

---

### 2) `domainKnowledge` section
The LLM can request additional domain knowledge markdown files.

**Where they live**
- Markdown files live in `KnowledgeBase/`
- `api/aime/route.js` contains `knowledgeBaseMap` mapping:
  - `id`
  - `description`
  - `filename`

#### Update base system prompt
Update the existing base system prompt to include the list of knowledge items (`id` + `description`) from `knowledgeBaseMap` so the LLM knows what it can request.

#### Backend behavior (`api/aime/route.js`)
When processing a request, if the incoming message history includes a `req_more_info` tool call containing `domainKnowledge.ids[]`:

- Load the corresponding markdown file(s)
- Attach them into the system prompt **only if not already attached**

**No duplicates**
Implement a clear unique marker for each attached KB file, e.g.:

- `<!--KB:ID=XYZ--> ...content... <!--/KB-->`

This allows the backend to detect and avoid attaching the same KB content twice.

---

### 3) `tools` section
The LLM can request additional tool schemas required to fulfill the user request.

**Where they live**
- Tool schema JSON files live in `ToolSchemas/`
- `api/aime/route.js` contains `toolMap` mapping:
  - `id`
  - `description`
  - `filename`

#### Backend behavior
When `req_more_info.tools.ids[]` is present:
- Load the referenced tool schema JSON file(s)
- Attach them to the tool list / prompt context as needed
- **Never attach the same tool schema twice**

Use a unique marker similar to KB (or track via a Set during request assembly), e.g.:
- `<!--TOOLDEF:ID=ABC--> ... <!--/TOOLDEF-->`
or an internal `attachedToolIds` set per request.

---

## End-to-end operational flow

1. User submits a query.
2. Backend sends base system prompt (plus any already-attached KB/tools) to the LLM.
3. If the LLM lacks context, it emits a `req_more_info` tool call.
4. Frontend intercepts the tool call, attaches requested `mainTree` slices (token minimal), then resends the same query.
5. Backend inspects the conversation for `req_more_info` requests:
   - attaches requested KnowledgeBase markdown (if missing)
   - attaches requested tool schemas (if missing)
6. LLM answers normally.

---

## Quality constraints / invariants

- Keep token usage minimal: request and attach only what is necessary.
- Never attach KnowledgeBase or tool schema files twice.
- Data attachments must be refreshable (remove + reattach to ensure freshness and incorporate newly requested fields).
- Keep the design compatible with future providers (`openAIHelper`, `ollamaHelper`) using the same `req_more_info` mechanism.



Later had to change the main tree triming requirement becase it was complecating and error prone. Now onwards, the entire section 
from mainTree (myOKRs or sharedOKRs gete appended without selectively attaching)
